# Complexity Notes



### Average case NP

#### reduction

Average case reduction中，第三条是单向的保护，其只保证D分布中，高概率的实例映射后不能变成D'分布中的低概率实例，但是并不保证D中低概率实例不被映射成D'中的高概率实例。

初衷：我们希望仅当L',D'在平均意义上比L,D更加困难时才允许归约。

首先，L中的困难实例映射后一定是L’中的困难实例。否则可以通过归约解决，说明其并不困难。

其次，我们要人为地阻止将一个困难实例高概率出现的分布映射为一个困难实例很低概率出现的分布，这不符合归约定义的意图。

但是我们允许将一个困难实例低概率出现的分布映射为一个困难实例更高概率出现的分布，因为这样显然后者更困难，符合归约的意图。尽管这时，原本容易的问题可能因为约束不得不被映射为困难的问题。

reduction是把原问题、分布映射为新问题的某个子集上的新分布。



#### distNP complete problem

这里仅用log比特表示M是因为，大部分01表示都无法解码出正常的TM，而拒绝一切的TM是容易判定的简单实例，因此为了防止整体分布过于简单，我们需要削弱解码失败造成的影响。在log比特表示下，这一概率仅仅是$\frac {1}{2^{log(n)}}$级别的，因此在average case reduction条件下是合理的。



#### peak elimination

对于长度固定的|x|,上述构造的问题采样是几乎均匀随机的，那么如果原分布D中有某个元素概率大于$2^{-n}$，那么我们需要先将其编码为某个更加均匀的分布。巧妙的是，这一步并不是归约（因为不满足长度正则性），而是植入在算法中的一部分。

事实上，当允许更宏观地考虑|x|可能变化时，上述distNP complete problem三元组的分布是极度不均匀的。而正是这种不均匀的问题空间使得接受不均匀的原问题成为了可能。

peak elimination并没有实质上地改变问题的真正分布，他只是提供了一种从非均匀的原分布到新问题空间的某个不均匀子分布的一种手段。



#### sampNP-complete

和distNP定义类似，不过sampNP的范围更大一些。然而可以证明，distNP-complete也是sampNP-complete的。



### Hardness amplification

$f$ 是一个 $\{0,1\}^n \to \{0, 1\}$ 的函数。

#### $\rho-$average hardness

平均难度接受两个参数，$\rho, n$， 前者表示难度（成功率），后者表示输入串长度。

$H_{avg}^{\rho}(f_n) = S$ 其中$S$可以理解为最小的能够以不低于概率 $\rho$ 的成功率计算$f$在 $\{0,1\}^n$上输出的电路尺寸.

#### worst-case hardness

最坏情况难度的定义是类似的，直接取$\rho=1$即可，其表示不超过该大小的电路都没法完全计算$f_n$

#### average hardness

最大的$S$，使得所有不超过$S$大小的电路都无法以超过 $1/2 + 1/S$的概率计算 $f_n$

这个定义是单调的，若对于某个 $S_0$,  存在小于 $S_0$的电路以超过 $1/2 + 1/S_0$的概率计算 $f_n$， 那么对于超过 $S_0$ 的 $S$，该电路的成功率也是超过 $1/2 + 1/S$的。

最后的 $S$ 越大，说明 $f_n$ 越接近于随机函数。



#### Yao's XOR Lemma

对于一个f，通过k次计算f，并将结果异或，可以得到一个更接近随机函数（需要更多门来计算）的函数。因为异或只要有一个错误就会造成总体错误，从而使得容错率变得更低。

证明引用了Impagliazzo's Hardcore Lemma。主要是将均匀分布拆分成一个简单分布和一个困难分布（由Impagliazzo's Hardcore Lemma给出）的叠加。叠加指的是，采样x前先抛一枚不均匀硬币。抛中正面就从简单分布中采样，抛中反面就从困难分布中采样。

注：这个叠加中，H前的系数是\delta。如果更大，那么G可能出现负概率，从而G定义不良。如果更小是可以的，但是相对的\epsilon更大了，即定理更弱了。

当命题中的\epsilon足够大（大于抽中简单分布的概率）时，采样的k个输入至少有一个来自困难分布。将其他变量的概率性改为存在性，从而有一个小电路能够以大于1/2 + 1/2 \epsilon的概率计算来自困难分布的问题。

#### Impagliazzo's Hardcore Lemma

S是最小的能以1-\delta 概率计算f的电路大小。这说明对于远小于S的电路存在公共的困难问题。否则可以通过拼接电路从而以更高的概率计算f。

存在一个比较平稳的分布（\delta-density），使得其主要概率集中在这些公共困难问题上，使得远小于S的电路总体难度变得极高。

其证明采用反证法，假设对于任何\delta-density 分布，都存在小电路能以超过\epsilon + 1/2 概率计算f，那么存在一个大电路以超过1-\delta概率计算f。

证明中的主要技巧包括：博弈论中的minmax定理，以此证明了存在一个小电路分布distC，对于任何\delta-density分布，从distC中采样都能以大于\epsilon + 1/2 概率计算f。此后通过划分x的好坏（计算难度）以及重复+投票证明，对于好的（简单的）x，存在一个大电路完美计算f(x)，并且坏的x占比小于\delta。从而这个大电路能以1-\delta概率计算f。



#### error correcting code

Walsh-Hadamard code 可以视作Reed-Muller code的线性版本。通过简单的傅立叶分析可知其码距为1/2（线性函数构成标准正交基，因此E[f(x)g(x)] = 1-2dist = 0。其unique local decoding 可采用简单的f(e_i) + f(x+e_i)，解码半径是1/4. 其列表解码采用Goldreich-levin学习算法，解码半径接近1/2。

Reed-Solomon code 也是 Reed-Muller code的单变量变体。一般采用大域，从而能够使用schwartz-zipple lemma计算码距。其将输入视为单变量多项式的系数，从而获得函数f。输出则是选择m个域中元素，计算其函数值。码距为1-d/m，d是多项式次数。RS码的unique decoding非常有趣，其将f视为C(x)/E(x)（有理函数）。然后强制令C和E在差错点（不超过d个）上都为0，从而使得这些点无效化，即无法影响多项式插值。此后再使用插值法解出C和E，得到f。

Reed Muller code。这里仍然考虑大域上的RM码。local decoder希望知道x处的值f(x)，那么可以先选择一条直线L = x + tz, z是方向向量。接着改变t得到一系列(t,f(t))对。注意到这是一个单变量多项式，因此可以通过RS decoder得到f限制在L上的函数Q(t)。带入t=0即得到f(x)。小域的Reed Muller local decoder 类似Walsh-Hadamard code。不过d次多项式需要d次导数等于0，即需要在一个d+1次仿射子空间上求和。解码半径是2^-{d+1}。

local decoder允许我们对于指数码长也有多项式时间的解码算法。但是由于存储原因，并不会在工程中使用指数码长的方案。



#### hardness amplifaction

函数f的worst-case hardness是S(n)，那么存在函数g和常数c，使得g的0.99-average case hardness大于S(n/c)/n^c 对于足够大的n。

思路是将f编码为长度为N的字符串（系数->值枚举），然后映射为CN长的字符串（具备纠错码结构，码距0.02。S(n/c)中的c是这里来的）。将映射后的字符串视为{0,1}^{log(CN)}->{0,1}的函数g。这样如果有小电路能以0.99概率逼近g，则再加上一些纠错门电路和局部解码门电路即可。由于局部解码允许bounded error，因此最后要复制整套电路多次并进行投票（n^c是这里来的）。



#### List Decoding 

##### Johnson Bound

Johnson Bound的条件是（假设在q元域上编码）：

1. 码距 d 满足：$ (1-1/q)(1-\epsilon) < d < 1- 1/q$ （后者是plokin bound）

2. 解码半径 r 满足: $r < (1-1/q)(1-\delta)$ 其中 $\delta > \sqrt{\epsilon}$

   或者更精确的：$ r < (1-1/q)(1-\sqrt{1-\frac{qd}{q-1}})$

在上述条件下，列表大小是$O(1/\delta^2)$

##### List Decoding of RS Code (Sudan's Algorithm)

首先，g是一个d次多项式。我们有m个(x, g(x))对。需要获得一个多项式级别的列表，包含g。

其想法和Unique decoding类似。在unique decoding中，我们构造了Q(x,y) = C(x)-E(x)y,这里我们将y的次数扩展到更高，$Q(x,y) = P_0(x) + P_1(x)y + P_2(x)y^2 ...$，并通过(a,b)点对拟合其系数。

共有m个方程。为了使得拟合成功，系数应该大于m，因此deg(Q)不能过小。这里取x最高次为sqrt{md}, y最高次为 sqrt{m/d}.从而有(sqrt{md}+1)(sqrt{m/d} + 1)个系数，大于m，一定有解。

当带入y=g(x)，当b=g(a)时，h(a)=Q(a,g(a))=0。有m-\delta个零点（\delta是错误数据数量）。因此我们可以控制Q的次数，当deg(h) < m-\delta时，h必须恒等于0 。从而Q中必须包含因子y-g(x)。因此deg(Q)不能过大。此外，该算法会解码出所有与接受数据距离小于m-deg(h)的其他多项式。

这里的Sudan算法可以返回所有\delta <m- 2\sqrt{dm} 的原始数据, 即解码半径是m - 2\sqrt{dm}.

由于Q中y的次数最高被限制为sqrt{m/d},因此列表大小最多是sqrt{m/d}。

想要获得更大的解码半径，需要deg(Q)更小。但是Sudan算法已经优化到极限了，因此需要新的算法。

思路是，让每个没有错误的点不仅只提供一个零点，我们要求其是Q的r重根。这样一个正确的点就能提供r个零点。但是要让一个多项式在(a,b)处有r重根，需要在这个点上满足r(r+1)/2个线性方程（包含多次导数=0）。这导致Q的次数变高了。不过好消息是，即使如此，deg（h）的增长速率还是略小于根数量的增长速率，从而我们获得了更大的解码半径m - \sqrt{dm}。

核心是次数控制下，系数、方程数、根数之间的权衡。

##### List Local Decoding of RM Code

算法的思路仍然是归约到RS解码。

设原多项式是P，次数为d，变量数为l，接收到的多项式是f。

目标是解码某一点x处的P(x)。随机选取x_0，随机选择一条三次曲线q,满足q(0)=x, q(r) = x_0。从而获取三次曲线上的点对(t,q(t))。可以证明，三次曲线上的点对是两两独立的，这允许我们使用chebyshev bound。

q是F到F^l的多项式。q(t) = (q_1(t), q_2(t), ...)每个分量都是一个三次多项式。P限制在q上，也就是用q_i(t)代入x_i,从而得到一个次数是3d的单变量多项式g(t) = P(q(t))。

同样的，f限制在q上得到g'(t) = f(q(t))的一系列数据。

然后用RS解码算法解码出g(0)即可。（解码得到一个列表。验证列表中g(r) = P(x_0)。)

